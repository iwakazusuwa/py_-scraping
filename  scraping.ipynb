{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d504a-7d90-4f86-a7ca-2a967339ac29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8379eaa7-3a4c-4e85-8419-1bc151e29c67",
   "metadata": {},
   "source": [
    "# 画像をスクレイピングしながら画像URL一覧ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98db548-5609-44be-a88a-7ef320066ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# 検索ワード\n",
    "#=============================================\n",
    "a='ペンギン 親子 かわいい'\n",
    "b='子猫 へそ天 いやし'\n",
    "c='かき氷 フルーツ エモイ'\n",
    "\n",
    "keywords = [a, b, c]\n",
    "\n",
    "#=============================================\n",
    "# 取得する画像数\n",
    "#=============================================\n",
    "kazu=5\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "#=============================================\n",
    "# 呼び出す\n",
    "#=============================================\n",
    "from icrawler.builtin import BingImageCrawler\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "from icrawler import ImageDownloader\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as selenium_logger \n",
    "\n",
    "from logging import getLogger, StreamHandler, DEBUG\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "\n",
    "#=============================================\n",
    "# Topディレクトリ名は処理開始時間にする\n",
    "#=============================================\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST=datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)\n",
    "day_foi = format(now, '%Y%m%d9%H%M%S')\n",
    "\n",
    "#=============================================\n",
    "# ディレクトリ作成\n",
    "#=============================================\n",
    "foi = './image_'+str(day_foi)\n",
    "os.makedirs(foi) # \n",
    "\n",
    "#=============================================\n",
    "# 画像ファイルと画像を取得したURLのリストファイル名\n",
    "#=============================================\n",
    "save_name = '/リスト.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebfec95-f493-4bb5-a27b-1a246340e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=============================================\n",
    "# 画像を保存しながら、URLと画像名をリストファイルに書き込む\n",
    "#=============================================\n",
    "\n",
    "class URLDownloader(ImageDownloader):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        import logging\n",
    "        self.logger.setLevel(logging.CRITICAL)    \n",
    "    \n",
    "    def save_column(self, folname, filepath, file_url, output_csv_path=None):\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = foi + save_name\n",
    "        with open(output_csv_path, 'a') as f:\n",
    "            output_str = f'{folname}, {filepath}, {file_url}\\n'\n",
    "            f.write(output_str)\n",
    "\n",
    "    def download(self, task, default_ext, timeout=5, max_retry=3, overwrite=False, **kwargs):\n",
    "        file_url = task['file_url']\n",
    "        task['success'] = False\n",
    "        task['filename'] = None\n",
    "        retry = max_retry\n",
    "\n",
    "        while retry > 0 and not self.signal.get('reach_max_num'):\n",
    "            try:\n",
    "                if not overwrite:\n",
    "                    with self.lock:\n",
    "                        self.fetched_num += 1\n",
    "                        filename = self.get_filename(task, default_ext)\n",
    "                        if self.storage.exists(filename):\n",
    "                            self.logger.info('skip downloading file %s', filename)\n",
    "                            return\n",
    "                        self.fetched_num -= 1\n",
    "\n",
    "                response = self.session.get(file_url, timeout=timeout)\n",
    "\n",
    "                if self.reach_max_num():\n",
    "                    self.signal['reach_max_num'] = True\n",
    "                    break\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    self.logger.error('Response status code %d, file %s',\n",
    "                                      response.status_code, file_url)\n",
    "                    break\n",
    "\n",
    "                if not self.keep_file(task, response, **kwargs):\n",
    "                    break\n",
    "\n",
    "                with self.lock:\n",
    "                    self.fetched_num += 1\n",
    "                    filename = self.get_filename(task, default_ext)\n",
    "\n",
    "                self.logger.info('image #%s\\t%s', self.fetched_num, file_url)\n",
    "                self.storage.write(filename, response.content)\n",
    "                task['success'] = True\n",
    "                task['filename'] = filename\n",
    "                #folname = task.get('folname', 'unknown')\n",
    "                self.save_column(folname, filename, file_url)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error('Exception caught when downloading file %s, error: %s, remaining retry times: %d',\n",
    "                                  file_url, e, retry - 1)\n",
    "            finally:\n",
    "                retry -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f81f902-4097-4b40-ac2e-384ad8f83233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 15:39:32,466 - INFO - icrawler.crawler - start crawling...\n",
      "2025-07-07 15:39:32,474 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-07-07 15:39:32,487 - INFO - feeder - thread feeder-001 exit\n",
      "2025-07-07 15:39:32,492 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2025-07-07 15:39:32,506 - INFO - icrawler.crawler - starting 1 downloader threads...\n",
      "2025-07-07 15:39:33,121 - INFO - parser - parsing result page https://www.bing.com/images/async?q=ペンギン 親子 かわいい&first=0\n",
      "2025-07-07 15:39:35,415 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2025-07-07 15:39:35,417 - INFO - parser - thread parser-001 exit\n",
      "2025-07-07 15:39:39,695 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-07-07 15:39:39,728 - INFO - icrawler.crawler - start crawling...\n",
      "2025-07-07 15:39:39,732 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-07-07 15:39:39,738 - INFO - feeder - thread feeder-001 exit\n",
      "2025-07-07 15:39:39,748 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2025-07-07 15:39:39,757 - INFO - icrawler.crawler - starting 1 downloader threads...\n",
      "2025-07-07 15:39:40,249 - INFO - parser - parsing result page https://www.bing.com/images/async?q=子猫 へそ天 いやし&first=0\n",
      "2025-07-07 15:39:50,452 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2025-07-07 15:39:50,463 - INFO - parser - thread parser-001 exit\n",
      "2025-07-07 15:39:57,033 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-07-07 15:39:57,076 - INFO - icrawler.crawler - start crawling...\n",
      "2025-07-07 15:39:57,078 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-07-07 15:39:57,083 - INFO - feeder - thread feeder-001 exit\n",
      "2025-07-07 15:39:57,095 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2025-07-07 15:39:57,109 - INFO - icrawler.crawler - starting 1 downloader threads...\n",
      "2025-07-07 15:39:57,617 - INFO - parser - parsing result page https://www.bing.com/images/async?q=かき氷 フルーツ エモイ&first=0\n",
      "2025-07-07 15:40:07,648 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2025-07-07 15:40:07,649 - INFO - parser - thread parser-001 exit\n",
      "2025-07-07 15:40:13,347 - INFO - icrawler.crawler - Crawling task done!\n"
     ]
    }
   ],
   "source": [
    "#=============================================\n",
    "# 順番に検索します\n",
    "#=============================================\n",
    "crawler = BingImageCrawler(storage ={'root_dir' : foi})\n",
    "\n",
    "for keyword in keywords:\n",
    "    if keyword == a:\n",
    "        moji = '1_'\n",
    "    elif keyword == b:\n",
    "        moji = '2_'\n",
    "    elif keyword == c:\n",
    "        moji = '3_'\n",
    "    # 検索ワードの頭に連番。VBAで使用するので    \n",
    "    folname = moji + keyword\n",
    "    \n",
    "    crawler = BingImageCrawler(downloader_cls=URLDownloader, storage={'root_dir': foi + '/'+ folname})   \n",
    "    crawler.crawl(keyword = keyword, max_num = kazu) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e106b56b-530b-448f-8a80-ac38f106dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#　Topデレクトリとリストファイルを開く\n",
    "#=============================================\n",
    "os.startfile(os.path.realpath(foi))\n",
    "os.startfile(os.path.realpath(foi) + save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
